{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AswinTony2001/FinalYearProject/blob/main/Caption_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fxErhXb1MYw",
        "outputId": "da868ab1-8833-490d-d83e-e42aa5f054ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfC3L1G1SI8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoKWM_cD1SLu"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_inceptionV3():\n",
        "    model = InceptionV3(weights='imagenet')\n",
        "    model2 = Model(model.input, model.layers[-2].output)\n",
        "    return model2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU_noZ1g5jFf"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iJMkNLx5zXA",
        "outputId": "f736d8fa-912c-4ad2-d78e-94012d588586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'image-captioning'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 270 (delta 13), reused 155 (delta 13), pack-reused 115\u001b[K\n",
            "Receiving objects: 100% (270/270), 12.71 MiB | 39.57 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Filtering content: 100% (9/9), 319.66 MiB | 69.29 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "#@title import libraries\n",
        "from PIL import Image\n",
        "    \n",
        "import json\n",
        "import os, shutil\n",
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "! git clone https://huggingface.co/spaces/NicolasVana/image-captioning.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AHdLbwc6etq"
      },
      "source": [
        "**Models Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdnkkmpt1SYD"
      },
      "outputs": [],
      "source": [
        "\n",
        "root = Path('image-captioning')\n",
        "aux_pre = root / 'Inception' / 'PretrainedInceptionLSTM'\n",
        "aux_re = root / 'Inception' / 'RetrainedInceptionLSTM'\n",
        "\n",
        "model_re_path = root / 'Inception' / 'RetrainedInceptionLSTM' / 'Model'\n",
        "model_inception_path = root / 'Inception' / 'RetrainedInceptionFeatureExtraction' / 'Model'\n",
        "model_pre_path = root / 'Inception' / 'PretrainedInceptionLSTM' / 'Model'\n",
        "\n",
        "\n",
        "\n",
        "# Must create\n",
        "\n",
        "def get_pretrained_inceptionV3():\n",
        "    model = InceptionV3(weights='imagenet')\n",
        "    model2 = Model(model.input, model.layers[-2].output)\n",
        "    return model2\n",
        "\n",
        "def fetch_auxiliary_files(type):\n",
        "    if type == 'Pretrained Inception':\n",
        "        word2Index = np.load(aux_pre / \"word2Index.npy\", allow_pickle=True).item()\n",
        "        index2Word = np.load(aux_pre / \"index2Word.npy\", allow_pickle=True).item()\n",
        "        variable_params = np.load(aux_pre / \"variable_params.npy\", allow_pickle=True).item()\n",
        "        return word2Index, index2Word, variable_params\n",
        "    if type == 'Retrained Inception':\n",
        "        word2Index = np.load(aux_re / \"word2Index.npy\", allow_pickle=True).item()\n",
        "        index2Word = np.load(aux_re / \"index2Word.npy\", allow_pickle=True).item()\n",
        "        variable_params = np.load(aux_re / \"variable_params.npy\", allow_pickle=True).item()\n",
        "        return word2Index, index2Word, variable_params\n",
        "\n",
        "def fetch_model(type):\n",
        "    if type == 'Pretrained Inception':\n",
        "        model_pre = tf.keras.models.load_model(model_pre_path)\n",
        "        model_inc = get_pretrained_inceptionV3()\n",
        "        return model_inc, model_pre\n",
        "    if type == 'Retrained Inception':\n",
        "        model_re = tf.keras.models.load_model(model_re_path)\n",
        "        model_inc = tf.keras.models.load_model(model_inception_path)\n",
        "        return model_inc, model_re\n",
        "\n",
        "def preprocess_image_inception(image):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(mode=\"RGB\")\n",
        "\n",
        "    x = np.array(image)\n",
        "    x = np.expand_dims(x, axis = 0)\n",
        "    x = preprocess_input(x)\n",
        "    x = x.reshape(1, 299, 299, 3)\n",
        "\n",
        "    return x\n",
        "\n",
        "def extract_features(model, image):\n",
        "    features = model.predict(image, verbose = 0)\n",
        "    return features\n",
        "\n",
        "def generate_caption(model, features, max_len, word2Index, index2Word, beam_index = 3):\n",
        "    caption = beam_search(model, features, max_len, word2Index, index2Word, beam_index)\n",
        "    return caption\n",
        "\n",
        "def beam_search(model, features, max_len, word2Index, index2Word, beam_index):\n",
        "    start = [word2Index[\"startseq\"]]\n",
        "    start_word = [[start, 1]]\n",
        "\n",
        "    final_preds = []\n",
        "    live_seqs = beam_index\n",
        "    features = np.tile(features, (beam_index,1))\n",
        "    count = 0\n",
        "    while len(start_word) > 0:\n",
        "        #print(count)\n",
        "        count+=1\n",
        "        temp = []\n",
        "        padded_seqs = []\n",
        "        #Get padded seqs for each of the starting seqs so far, misnamed as start_word\n",
        "        for s in start_word:\n",
        "            par_caps = pad_sequences([s[0]], maxlen=max_len, padding='post')\n",
        "            padded_seqs.append(par_caps)\n",
        "\n",
        "        #Formatting input so that it can be used for a prediction\n",
        "        padded_seqs = np.array(padded_seqs).reshape(len(start_word), max_len)\n",
        "\n",
        "        preds = model.predict([features[:len(start_word)],padded_seqs], verbose=0)\n",
        "\n",
        "        #Getting the best branches for each of the start seqs that we had\n",
        "        for index, pred in enumerate(preds):\n",
        "            word_preds = np.argsort(pred)[-live_seqs:]\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = start_word[index][0][:], start_word[index][1]\n",
        "                next_cap.append(w)\n",
        "                prob *= pred[w]\n",
        "                temp.append([next_cap, prob])\n",
        "\n",
        "        start_word = temp\n",
        "        # Sorting according to the probabilities\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words from all branches\n",
        "        start_word = start_word[-live_seqs:]\n",
        "\n",
        "        for pair in start_word:\n",
        "            if index2Word[pair[0][-1]] == 'endseq':\n",
        "                final_preds.append([pair[0][:-1], pair[1]])\n",
        "                start_word = start_word[:-1]\n",
        "                live_seqs -= 1\n",
        "            if len(pair[0]) == max_len:\n",
        "                final_preds.append(pair)\n",
        "                start_word = start_word[:-1]\n",
        "                live_seqs -= 1\n",
        "\n",
        "    # Between all the finished sequences (either max len or predicted endseq), decide which is best\n",
        "    max_prob = 0\n",
        "    for index, pred in enumerate(final_preds):\n",
        "        if pred[1] > max_prob:\n",
        "            best_index = index\n",
        "            max_prob = pred[1]\n",
        "\n",
        "    # Convert to readable text\n",
        "    final_pred = final_preds[best_index]\n",
        "    final_caption = [index2Word[i] for i in final_pred[0]]\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption\n",
        "\n",
        "# # create target model directory\n",
        "# model_dir = './models/'\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "#\n",
        "# files_to_download = [\n",
        "#     \"config.json\",\n",
        "#     \"flax_model.msgpack\",\n",
        "#     \"merges.txt\",\n",
        "#     \"special_tokens_map.json\",\n",
        "#     \"tokenizer.json\",\n",
        "#     \"tokenizer_config.json\",\n",
        "#     \"vocab.json\",\n",
        "#     \"preprocessor_config.json\",\n",
        "# ]\n",
        "\n",
        "def _compile():\n",
        "\n",
        "    image_path = 'image-captioning/samples/ROCO_00929.jpg'\n",
        "    image = Image.open(image_path)\n",
        "    #predict(image)\n",
        "    image.close()\n",
        "\n",
        "\n",
        "_compile()\n",
        "\n",
        "\n",
        "sample_dir = 'image-captioning/samples/'\n",
        "sample_image_ids = tuple([\"None\"] + [int(f.replace('ROCO_', '').replace('.jpg', '')) for f in os.listdir(sample_dir) if f.startswith('ROCO_')])\n",
        "\n",
        "with open(os.path.join(sample_dir, \"Roco-img-ids.json\"), \"r\", encoding=\"UTF-8\") as fp:\n",
        "    roco_image_ids = json.load(fp)\n",
        "\n",
        "\n",
        "def get_random_image_id():\n",
        "\n",
        "    image_id = random.sample(roco_image_ids, k=1)[0]\n",
        "    return image_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2gZRBhMh7Tt"
      },
      "outputs": [],
      "source": [
        "model_type = 'Pretrained Inception' # 'Retrained Inception'\n",
        "\n",
        "inception, lstm = fetch_model(model_type)\n",
        "inception.trainable=False\n",
        "# For freezing the layer we make use of layer.trainable = False\n",
        "# means that its internal state will not change during training.\n",
        "# model's trainable weights will not be updated during fit(),\n",
        "# and also its state updates will not run.\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "\t\tinception,\n",
        "    tf.keras.layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
        "    tf.keras.layers.Dense(7184, activation='sigmoid', name='output')\n",
        "\t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzIpigT2h7Xk",
        "outputId": "d42db6b6-ff7b-4180-a4d7-bd18a5c810de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_1 (Functional)        (None, 2048)              21802784  \n",
            "                                                                 \n",
            " hidden_layer (Dense)        (None, 1024)              2098176   \n",
            "                                                                 \n",
            " output (Dense)              (None, 7184)              7363600   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,264,560\n",
            "Trainable params: 9,461,776\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvQmcePrjfUz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# deep learning libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import applications\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
        "from keras.preprocessing import image\n",
        "\n",
        "import cv2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7izDwJMXjfZM",
        "outputId": "a979d8bd-682c-45c2-a375-a93caa57103b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "url = '/content/drive/MyDrive/Final Project Dataset'\n",
        "# datasets\n",
        "labels = pd.read_csv(url+\"/Detection/Final Training Detection.csv\")\n",
        "sample = pd.read_csv(url+'/Detection/Final Training Detection.csv')\n",
        "label2 = pd.read_csv(url+\"/Detection/Final Testing Detection.csv\")\n",
        "\n",
        "# folders paths\n",
        "train_path = url+\"/Common/Training/Final Training Images\"\n",
        "test_path = url+\"/Common/Testing/Final Testing Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIuygAhcjfcS"
      },
      "outputs": [],
      "source": [
        "classes = [\"cuis\"]\n",
        "for i in range(2,4):\n",
        "  classes.append(f\"Unnamed: {i}\")\n",
        "labels = labels.astype(object).replace(np.nan, 'None')\n",
        "labels[\"CombinedColumns\"] = labels[classes].apply(lambda x: x.tolist(), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n4rNeIdjffr"
      },
      "outputs": [],
      "source": [
        "def to_jpg(id):\n",
        "\treturn id+\".jpg\"\n",
        "\n",
        "\n",
        "labels['ID'] = labels['ID'].apply(to_jpg)\n",
        "sample['ID'] = sample['ID'].apply(to_jpg)\n",
        "label2['ID'] = label2['ID'].apply(to_jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUUeuM8Cjfis",
        "outputId": "cf49ad62-e5ad-4f03-8dd8-837584311b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 64995 validated image filenames belonging to 7184 classes.\n",
            "Found 65000 validated image filenames belonging to 7184 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data agumentation and pre-processing using tensorflow\n",
        "gen1 = ImageDataGenerator(\n",
        "\t\t\t\trescale=1./255.\n",
        "\t\t\t\t)\n",
        "\n",
        "gen2 = ImageDataGenerator(\n",
        "\t\t\t\trescale=1./255.\n",
        "\t\t\t\t)\n",
        "\n",
        "train_generator = gen1.flow_from_dataframe(\n",
        "\tlabels, # dataframe\n",
        "\tdirectory = train_path, # images data path / folder in which images are there\n",
        "\tx_col = 'ID',\n",
        "\ty_col = 'CombinedColumns',\n",
        "\tsubset=\"training\",\n",
        "\tcolor_mode=\"rgb\",\n",
        "\ttarget_size = (255,255), # image height , image width\n",
        "\tclass_mode=\"categorical\",\n",
        "\tbatch_size=10,\n",
        "\tshuffle=True,\n",
        "\tseed=42,\n",
        ")\n",
        "\n",
        "\n",
        "validation_generator = gen2.flow_from_dataframe(\n",
        "\tlabels, # dataframe\n",
        "\tdirectory = train_path, # images data path / folder in which images are there\n",
        "\tx_col = 'ID',\n",
        "\ty_col = 'CombinedColumns',\n",
        "\tsubset=\"training\",\n",
        "\tcolor_mode=\"rgb\",\n",
        "\ttarget_size = (255,255), # image height , image width\n",
        "\tclass_mode=\"categorical\",\n",
        "\tbatch_size=10,\n",
        "\tshuffle=True,\n",
        "\tseed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lavz6LaQjfkY"
      },
      "outputs": [],
      "source": [
        "def macro_f1(y, y_hat, thresh=0.5):\n",
        "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
        "    \n",
        "    Args:\n",
        "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
        "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
        "        thresh: probability value above which we predict positive\n",
        "        \n",
        "    Returns:\n",
        "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
        "    \"\"\"\n",
        "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
        "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
        "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
        "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
        "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
        "    macro_f1 = tf.reduce_mean(f1)\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nHD816yj7xn"
      },
      "outputs": [],
      "source": [
        "LR = 1e-10 # Keep it small when transfer learning\n",
        "EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KenlNi3Cj70p"
      },
      "outputs": [],
      "source": [
        "model1.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSzffOUQj73j",
        "outputId": "9e63868c-ee8a-4d4a-ba3c-2e2411f0d130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 26.4059 - accuracy: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "history = model1.fit(train_generator,   #### the above commented version is ur code\n",
        "\t\t\t\t\tsteps_per_epoch=10,\n",
        "\t\t\t\t\tvalidation_data=validation_generator,\n",
        "\t\t\t\t\tvalidation_steps=10,\n",
        "\t\t\t\t\tepochs=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVVdNjMR1Sds",
        "outputId": "f9e1a1aa-6535-46b7-dde3-acff5c9c6abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model_type = 'Pretrained Inception' # 'Retrained Inception'\n",
        "\n",
        "inception, lstm = fetch_model(model_type)\n",
        "word2Index, index2Word, variable_params = fetch_auxiliary_files(model_type)\n",
        "max_len = variable_params['max_caption_len']\n",
        "\n",
        "\n",
        "sample_name = \"ROCO_00001.jpg\"\n",
        "sample_dir = \"/content/image-captioning/samples/\"\n",
        "sample_path = os.path.join(sample_dir, sample_name)\n",
        "image = Image.open(sample_path)\n",
        "\n",
        "width, height = 299, 299\n",
        "resized = image.resize(size=(width, height))\n",
        "\n",
        "preprocessed_img = preprocess_image_inception(resized)\n",
        "features = extract_features(inception, preprocessed_img)\n",
        "caption = generate_caption(lstm, features, max_len, word2Index, index2Word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MJQFYG7l1eBo",
        "outputId": "60f3469f-900d-4a15-8317-8b4535a821fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0d5e889-8f56-47f0-8148-4deffd677279\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0d5e889-8f56-47f0-8148-4deffd677279\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving ImageCLEFmedCaption_2022_train_065404.jpg to ImageCLEFmedCaption_2022_train_065404.jpg\n"
          ]
        }
      ],
      "source": [
        "def upload_files():\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(k, 'wb').write(v)\n",
        "  return list(uploaded.keys())\n",
        "files = upload_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR_iO7V_1eEG"
      },
      "outputs": [],
      "source": [
        "def predict(img_path):\n",
        "  sample_path = img_path\n",
        "  image = Image.open(sample_path)\n",
        "  width, height = 299, 299\n",
        "  resized = image.resize(size=(width, height))\n",
        "\n",
        "  preprocessed_img = preprocess_image_inception(resized)\n",
        "  features = extract_features(inception, preprocessed_img)\n",
        "  caption = generate_caption(lstm, features, max_len, word2Index, index2Word)\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmnhumeU1eGs",
        "outputId": "36f17c3a-d6f4-48d0-ade4-e8af14506e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Caption --> [[0.9060744  0.1595825  0.2472212  ... 0.5204472  1.4509407  0.28606784]]\n"
          ]
        }
      ],
      "source": [
        "pred = predict('ImageCLEFmedCaption_2022_train_065404.jpg')\n",
        "#ref = 'brain and carotid artery magnetic resonance angiography show that the main vessel be normal'\n",
        "#print('Original Caption --> ', ref)\n",
        "print('Predicted Caption -->', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62NpJjKN1lsp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "def calculateBLEU(Actual_Answer,Generated_Answer):\n",
        "        pattern = re.compile(r'\\s+')\n",
        "        # NLTK\n",
        "        # Download Punkt tokenizer (for word_tokenize method)\n",
        "        # Download stopwords (for stopword removal)\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "            stops = set(stopwords.words(\"english\"))\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            stops = set(stopwords.words(\"english\"))\n",
        "        # Stemming\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "        # Remove punctuation from string\n",
        "        translator = str.maketrans(',-()', '    ')\n",
        "\n",
        "        # Define max score and current score\n",
        "        max_score = len(Actual_Answer)\n",
        "        current_score = 0.0\n",
        "\n",
        "        for i in range(len(Actual_Answer)):\n",
        "            candidate_caption = Generated_Answer[i].strip()\n",
        "            gt_caption = re.sub(pattern, ' ', Actual_Answer[i]).strip()\n",
        "            bleu_score = 0.0\n",
        "            if '#' not in gt_caption:\n",
        "                bleu_score = calc_single_blue_score(candidate_caption, gt_caption, translator, stops,\n",
        "                                                         stemmer)\n",
        "            else:\n",
        "                candidate_gt_captions = gt_caption.split(\"#\")\n",
        "                bleu_scores_of_all_possibilities = []\n",
        "                for gt_caption in candidate_gt_captions:\n",
        "                    bleu_scores_of_all_possibilities.append(\n",
        "                        calc_single_blue_score(candidate_caption, gt_caption, translator, stops,\n",
        "                                                    stemmer))\n",
        "                bleu_score = max(bleu_scores_of_all_possibilities)\n",
        "\n",
        "            # Increase calculated score\n",
        "            current_score += bleu_score\n",
        "\n",
        "        return round(current_score / max_score, 4)\n",
        "\n",
        "def calc_single_blue_score(candidate_caption, gt_caption, translator, stops, stemmer):\n",
        "\n",
        "        # Split caption into individual words (remove punctuation)\n",
        "        candidate_words = nltk.tokenize.word_tokenize(candidate_caption.translate(translator))\n",
        "        gt_words = nltk.tokenize.word_tokenize(gt_caption.translate(translator))\n",
        "\n",
        "        candidate_words = [stemmer.stem(word) for word in candidate_words]\n",
        "        gt_words = [stemmer.stem(word) for word in gt_words]\n",
        "\n",
        "        # Calculate BLEU score for the current caption\n",
        "        try:\n",
        "            # If both the GT and candidate are empty, assign a score of 1 for this caption\n",
        "            if len(gt_words) == 0 and len(candidate_words) == 0:\n",
        "                bleu_score = 1\n",
        "            # Calculate the BLEU score\n",
        "            else:\n",
        "                bleu_score = nltk.translate.bleu_score.sentence_bleu([gt_words], candidate_words,\n",
        "                                                                     smoothing_function=SmoothingFunction().method0,weights=(1, 0, 0, 0))\n",
        "                #print(gt_words,\"----\",candidate_words,\"----\",bleu_score)\n",
        "        # Handle problematic cases where BLEU score calculation is impossible\n",
        "        except ZeroDivisionError:\n",
        "            pass\n",
        "            # raise Exception('Problem with {} {}', gt_words, candidate_words)\n",
        "        return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC58aGFj1lvO",
        "outputId": "4ef55994-8f21-4655-d299-544270a1ef5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU SCORE -->  0.1875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "print('BLEU SCORE --> ',calculateBLEU([ref],[pred]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQU5MR3H1lyE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER2ul79B1l0R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}